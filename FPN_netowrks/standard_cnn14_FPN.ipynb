{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to represent the audio in the format as computer understand and to make the model learn the important characteristics of the audio sample, we need to extract perfect values from the audio signals.\n",
    "There are mainly 2 types of feature engineering:\n",
    "1. hand made features \n",
    "2. using the direct method to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract MFCC features from the audio and create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating dataloaders\n",
    "source for the code: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "class Audiodataset(Dataset):\n",
    "    def __init__(self, audio_files, labels, transform=None, max_length=256):\n",
    "        self.audio_files = audio_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(audio_path, sr=32000)\n",
    "\n",
    "        # Filter out zero-length audio files\n",
    "        if len(y) == 0:\n",
    "            print(f\"Warning: {audio_path} has zero length.\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        # Convert to Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=64, fmin=50, fmax=14000)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        # Pad or trim to max_length\n",
    "        if log_mel_spectrogram.shape[1] < self.max_length:\n",
    "            pad_width = self.max_length - log_mel_spectrogram.shape[1]\n",
    "            log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            log_mel_spectrogram = log_mel_spectrogram[:, :self.max_length]\n",
    "\n",
    "        if self.transform:\n",
    "            log_mel_spectrogram = self.transform(log_mel_spectrogram)\n",
    "\n",
    "        log_mel_spectrogram = torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)  # Shape [1, time_steps, mel_bins]\n",
    "        log_mel_spectrogram = log_mel_spectrogram.permute(0, 2, 1)  # Shape [1, mel_bins, time_steps]\n",
    "        #print(f'shape of log_mel_spect: {log_mel_spectrogram}')\n",
    "        return log_mel_spectrogram, torch.tensor(label, dtype=torch.long)  # Ensure label is a long tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing the data for training using dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_dataloader=1552\n",
      "data loaders are ready for training step\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#define classes\n",
    "classes = ['Baby_Cry','Door_Knock','Door_Bell','Fire_Alarm']\n",
    "image_dir = \"C:\\\\Users\\\\PC\\\\Desktop\\\\lisnen_segmented_working_data\\\\audio_files\"\n",
    "\n",
    "audio_files = []\n",
    "labels = []\n",
    "\n",
    "for label, class_name in enumerate(classes):   #by using enumerate here we're saving index fo an element in label and its value in class_name\n",
    "    class_dir = os.path.join(image_dir,class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        # check if te file format is valid\n",
    "        if file_name.endswith('.wav'):  #coz we're working only on wav files\n",
    "            audio_files.append(os.path.join(class_dir,file_name))\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "# crating the data splits from audio_files to train, val and test splits\n",
    "training_audio,dummy_training_audio,training_labels,dummy_training_labels = train_test_split(audio_files,labels,test_size=0.2,shuffle=True,random_state=42)\n",
    "val_audio,test_audio,val_labels,test_labels = train_test_split(dummy_training_audio,dummy_training_labels,test_size=0.5,shuffle=True,random_state=42)\n",
    "\n",
    "# save the log mel values in the list format and then create dataloaders\n",
    "training_data  = Audiodataset(training_audio,training_labels,max_length=256)\n",
    "validation_data = Audiodataset(val_audio,val_labels,max_length=256)\n",
    "test_data = Audiodataset(test_audio,test_labels,max_length=256)\n",
    "\n",
    "# creating the data loaders\n",
    "train_dataloader = DataLoader(training_data,batch_size=16,shuffle = True)\n",
    "val_data_loader = DataLoader(validation_data,batch_size=16,shuffle=False)\n",
    "test_data_loader = DataLoader(test_data,batch_size=16,shuffle=False)\n",
    "print(f'len of train_dataloader={len(train_dataloader.dataset)}')\n",
    "print(\"data loaders are ready for training step\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining backbone network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a backbone cnn network \n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\PC\\\\Desktop\\\\lisnen_segmented_working_data\\\\Lisnen-research\\\\Lisnen-research\\\\audioset_tagging_cnn\\\\pytorch\")\n",
    "from models import Cnn14\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self,sample_rate,window_size,hop_size,mel_bins,fmin,fmax,classes_num):\n",
    "        super(Backbone, self).__init__()\n",
    "        # Initialize Cnn14\n",
    "        self.cnn14 = Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num)\n",
    "        # Replace the initial layers to accept Mel spectrograms directly\n",
    "        self.cnn14.spectrogram_extractor = nn.Identity()\n",
    "        self.cnn14.logmel_extractor = nn.Identity()\n",
    "        self.cnn14.fc1 = nn.Identity()\n",
    "        self.cnn14.fc_audioset = nn.Identity()\n",
    "        \n",
    "        # Adjust the SpecAugmentation parameters\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "                                               freq_drop_width=8, freq_stripes_num=2)\n",
    "        # input shape of tensor is (batch_size,no_of_channels,height,width)\n",
    "    def forward(self, x):\n",
    "        #print(f'sape before augmentation ={x.shape}')\n",
    "        x = self.spec_augmenter(x)\n",
    "        #print(f'input shape = {x.shape}')\n",
    "        c1 = self.cnn14.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        # as per cnn14 in_channels = 1, out_channels = 64\n",
    "        #print('shape of c1 ',c1.shape)\n",
    "        c2 = self.cnn14.conv_block2(c1, pool_size=(2, 2), pool_type='avg')\n",
    "        # as per cnn14 in_channels = 64, out_channels = 128\n",
    "        #print('shape of c2 ',c2.shape)\n",
    "        c3 = self.cnn14.conv_block3(c2, pool_size=(2, 2), pool_type='avg')\n",
    "        #print('shape of c3 ',c3.shape)\n",
    "        c4 = self.cnn14.conv_block4(c3, pool_size=(2, 2), pool_type='avg')\n",
    "        #print('shape of c4 ',c4.shape)\n",
    "        c5 = self.cnn14.conv_block5(c4, pool_size=(2, 2), pool_type='avg')\n",
    "        #print('shape of c5 ',c5.shape)\n",
    "        c6 = self.cnn14.conv_block6(c5,pool_size = (1,1), pool_type='avg')\n",
    "        #print('shape of c6 ',c6.shape)\n",
    "        return c1,c2, c3, c4, c5,c6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Feature pyramid network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FPNWithBackbone(nn.Module):\n",
    "    def __init__(self, backbone,num_classes):\n",
    "        super(FPNWithBackbone, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        # Lateral convolutions to match the number of channels from backbone layers\n",
    "        self.lateral_conv1 = nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0)  #64= input channels, 256=output channels\n",
    "        self.lateral_conv2 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0) #128= input channels,256=output channels\n",
    "        self.lateral_conv3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)  \n",
    "        self.lateral_conv4 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)   \n",
    "        self.lateral_conv5 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)   \n",
    "        self.lateral_conv6 = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)   \n",
    "\n",
    "        # Additional layers after FPN\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #final_output_layre\n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(256 * 64 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone forward pass\n",
    "        c1, c2, c3, c4, c5, c6 = self.backbone(x)\n",
    "        \n",
    "        # FPN layers\n",
    "        p6 = self.lateral_conv6(c6)  #lateral conv layer is used to equlaize no of channels\n",
    "        #print(f'shape of p6 ={p6.shape}')\n",
    "        p5 = self.lateral_conv5(c5) + p6\n",
    "        #print(f'shape of p5={p5.shape}')\n",
    "        p4 = self.lateral_conv4(c4) + F.interpolate(p5, scale_factor=2, mode='nearest')\n",
    "        #print(f'shape of p4={p4.shape}')\n",
    "        p3 = self.lateral_conv3(c3) + F.interpolate(p4, scale_factor=2, mode='nearest')\n",
    "        #print(f'shape of p3={p3.shape}')\n",
    "        p2 = self.lateral_conv2(c2) + F.interpolate(p3, scale_factor=2, mode='nearest')\n",
    "        #print(f'shape of p2={p2.shape}')\n",
    "        p1 = self.lateral_conv1(c1) + F.interpolate(p2, scale_factor=2, mode='nearest')\n",
    "        #print(f'shape of p1={p1.shape}')\n",
    "        \n",
    "        # Additional layers\n",
    "        out = self.conv1(p1)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.max_pool(out)\n",
    "\n",
    "        # Flatten and pass through the final fully connected layer\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        out = self.fc(out)  # No activation function here\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the required parameters\n",
    "sample_rate = 32000\n",
    "window_size = 1024\n",
    "hop_size = 320\n",
    "mel_bins = 64\n",
    "fmin = 50\n",
    "fmax = 14000\n",
    "classes_num = 4  # Number of classes in your dataset\n",
    "\n",
    "# Initialize the modified model\n",
    "backbone = Backbone(sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num)\n",
    "fpn_model = FPNWithBackbone(backbone,num_classes = classes_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPNWithBackbone(\n",
       "  (backbone): Backbone(\n",
       "    (cnn14): Cnn14(\n",
       "      (spectrogram_extractor): Identity()\n",
       "      (logmel_extractor): Identity()\n",
       "      (spec_augmenter): SpecAugmentation(\n",
       "        (time_dropper): DropStripes()\n",
       "        (freq_dropper): DropStripes()\n",
       "      )\n",
       "      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv_block1): ConvBlock(\n",
       "        (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv_block2): ConvBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv_block3): ConvBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv_block4): ConvBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv_block5): ConvBlock(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv_block6): ConvBlock(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (fc1): Identity()\n",
       "      (fc_audioset): Identity()\n",
       "    )\n",
       "    (spec_augmenter): SpecAugmentation(\n",
       "      (time_dropper): DropStripes()\n",
       "      (freq_dropper): DropStripes()\n",
       "    )\n",
       "  )\n",
       "  (lateral_conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lateral_conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lateral_conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lateral_conv4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lateral_conv5): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lateral_conv6): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=262144, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading pre trained weights\n",
    "\n",
    "pretrained_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\lisnen_segmented_working_data\\\\CNN_files\\\\Cnn14_mAP=0.431.pth'  # Update this path to your pretrained model file\n",
    "checkpoint = torch.load(pretrained_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the pretrained weights, except for the final layer\n",
    "model_dict = backbone.state_dict()\n",
    "pretrained_dict = {k: v for k, v in checkpoint['model'].items() if k in model_dict and 'fc_audioset' not in k}\n",
    "model_dict.update(pretrained_dict)\n",
    "backbone.load_state_dict(model_dict)\n",
    "fpn_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating training function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    accumulation_steps = 4\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            #print(f'shape of inputs={inputs.shape}')\n",
    "            #print(f'shape of labels={labels.shape}')\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)  # Outputs should be [batch_size, num_classes]\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Criterion expects outputs and labels in the correct shape\n",
    "            loss = loss / accumulation_steps  # Normalize loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(val_loader)\n",
    "        val_epoch_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_accuracy:.2f}%\")\n",
    "\n",
    "        # Save the model if the validation accuracy is the best we've seen so far.\n",
    "        if val_epoch_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_epoch_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Model saved with validation accuracy: {val_epoch_accuracy:.2f}%\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "\n",
    "#train_losses, train_accuracies, val_losses, val_accuracies \n",
    "def visualize(train_accuracies,val_accuracies,train_losses,val_losses,num_epochs):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/97 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11432\\2422389930.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11432\\69067382.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m  \u001b[1;31m# Normalize loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PC\\.conda\\envs\\pretrained\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\PC\\.conda\\envs\\pretrained\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Initialize CNN14 backbone\n",
    "#backbone = Backbone_Cnn14(sample_rate=32000, window_size=1024, hop_size=320, mel_bins=32, fmin=50, fmax=14000, classes_num=4)\n",
    "\n",
    "# Initialize FPN\n",
    "#model = FPNWithBackbone(backbone)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fpn_model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce lr by 0.1 every 10 epochs\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train(fpn_model, train_dataloader, val_data_loader, criterion, optimizer, scheduler,num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained",
   "language": "python",
   "name": "pretrained"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
